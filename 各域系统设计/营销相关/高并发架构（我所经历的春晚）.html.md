---
author: zouzq7@163.com
created: "2019-02-19 11:25:31 +0000"
exporter-version: Evernote Mac 9.6.4 (470194)
source: desktop.mac
title: 高并发架构（我所经历的春晚）
updated: "2019-02-21 08:27:15 +0000"
---

<div>

有幸参与了2019百度春晚活动，体验了下高并发难度和快感。我当时参与的是发券相关模块的开发，独立负责了一部分预案策略、灌券的模块。之后又稍带做了元宵节活动，之前也参与过dsp的开发，对高并发场景算是有了一点了解，觉着有一些知识和经验需要沉淀下来了。这一篇算是背景相关的描述，并且关于难点及问题的阐述吧。

</div>

<div>

记得之前看过一篇腾讯的文章叫做：微信春晚红包的10亿个红包，技术架构到底难在哪儿？（<https://www.cnblogs.com/kunshandajiake/p/4322529.html>）大家有兴趣可以看下。

</div>

<div>

下面就站在发券这边的立场上进行阐述了，因为涉及到公司内部的方案，这里不会对一些细节进行描述，仅说明一些痛点和我们所想到的解决方案。

</div>

<div>

\

</div>

<div>

那天比较难的一个最大的点是 那几轮口播发生时的流量高峰：千万qps

</div>

<div>

![](%E9%AB%98%E5%B9%B6%E5%8F%91%E6%9E%B6%E6%9E%84%EF%BC%88%E6%88%91%E6%89%80%E7%BB%8F%E5%8E%86%E7%9A%84%E6%98%A5%E6%99%9A%EF%BC%89.resources/6835CD70-3061-4655-A9C6-E2959F545384.png) 
 \

</div>

<div>

首先是接入层，接入层如果挂掉整个活动都没得玩了，所以我们要选一种合适的方式来扛住打到接入层的流量，最常见的就是nginx +
lua 保证接入层的正常运作，后面会有单独的篇章来说一下Nginx + lua
是如何抗住高qps的。

</div>

<div>

\

</div>

<div>

接入层抗住了，我们就完成第一个目标，但是后续如果按照这个qps直接打到业务系统上，很多情况下，即使堆机器也不能有效的解决问题（通常情况下
机器没这么多、业务系统一般或多或少都有瓶颈），我们需要优化现有的业务系统，在这种时间紧压力大的活动上，临时大改业务系统是不可能的（所以平时写业务系统的时候就要做好充分的优化，尽可能的减少瓶颈），我们可以在一些技术点上做优化，比如说线程优化，较少IO操作等，做好一些非必须服务的降级，避免不必要的开销。在优化尽可能做好之后，其实还是有一个能抗多少qps的瓶颈存在的。

</div>

<div>

\

</div>

<div>

面对业务系统的漫长耗时&接入层的未知流量相对于业务系统的最大承载能力，很显然我们需要异步化了。最近点的方式就是使用内存数据库Redis，Redis
最大的特点就是快，在这里我们可以写入Redis 即认为成功。

</div>

<div>

1、然后让业务系统去消费Redis 里面的数据

</div>

<div>

2、起一个单独的服务来消费Redis然后调用现有的业务系统，由这个消费服务来集中控制对于下游的qps。

</div>

<div>

很显然第二种方式是比较合适的，消费服务不涉及业务，业务系统专心处理业务，而且qps控制，消费频率控制这种服务服用能力也比较强，以后也可以方便使用。

</div>

<div>

\

</div>

<div>

下一个瓶颈是数据库了，很显然正常情况下使用Mysql方式的已经无法满足了，一张表数据量就大到惊人，如果再按部就班的存储数据库耗时会直线上升，尤其是在有锁的情况下。这时候数据库就需要做分片了，如何分片又是一个经典的问题，后面的文章会做具体的介绍。这里提一下，上面提到的Redis
也是要做分片的。分完片就很大程度上能摆脱单机的处理能力了。

</div>

<div>

\

</div>

<div>

画个时序图来看是这样的

</div>

<div>

![](%E9%AB%98%E5%B9%B6%E5%8F%91%E6%9E%B6%E6%9E%84%EF%BC%88%E6%88%91%E6%89%80%E7%BB%8F%E5%8E%86%E7%9A%84%E6%98%A5%E6%99%9A%EF%BC%89.resources/C1D5A167-B94C-447B-8F86-E62BBFCA831A.png) 
 \

</div>

<div>

这样理论上就能抗住高qps了（当然啦还需要很多细节上的优化，大致架构是这样的）。发券信息直接塞到redis
就认为成功直接返回，这一点在几十万qps下基本是没有问题的，但是如果qps再高就会偶尔出现写失败或者写超时的情况（这个问题可以利用接入层日志做延时补单来解决），但还有一个不好解决的问题，虽然针对发券模块能抗住这么高的qps
了，但对于上层直接调用发券接口的量很大启用大量的socket会导致链接过多而夯住，所以说我们需要灵活点分配下流量。

</div>

<div>

换一个思路，问题的点在于上游不能大量的调用发券接口，那我们可以采用更加粗暴的方式，发券写入日志&redis即认为成功，不调用发券接口。用户查看礼品时照常展示刚刚"发过的券"，当用户真实的去卡券包查看券或者使用时再真实发券。同时去异步消费上游发券日志来进行真实的发券，这样流量相当于就被打散了，一部分实际上是没有实时的打到发券模块上的。仅有查看券和使用券才会真的打到发券模块，这样相当于即减缓了上游压力，也减缓了下游压力。所以整个变成了这样：

</div>

<div>

![](%E9%AB%98%E5%B9%B6%E5%8F%91%E6%9E%B6%E6%9E%84%EF%BC%88%E6%88%91%E6%89%80%E7%BB%8F%E5%8E%86%E7%9A%84%E6%98%A5%E6%99%9A%EF%BC%89.resources/FD5D3FD5-C3F9-4F70-82A3-C0020382E26F.png) 
 \

</div>

<div>

这些做好之后，看似已经问题不大了，但实际上还有一个很大的问题：网络耗时，我们需要清楚的知道自己用了哪些机房，哪些服务是在哪些机房，具体到机器上是混布还是单独的，服务之间调用跨机房调用多吗？这些都是需要考虑和优化的。

</div>

<div>

当服务部署合理优化也到位了，面对千万的压力挂还是有可能的。所以说我们需要做一件很重要的事儿------预案

</div>

<div>

我本次负责的是redis
挂掉的预案，其实对于下游系统整体挂掉采用的预案也是相同的，nginx +
lua的正常保证了整个服务的正常对外服务，只不过无法正常的写入redis
进行消费了而已，这里我们就可以利用接入层日志进行消费直接控制好qps调用下游业务系统，我们需要实现下游业务模块的幂等性，保证可重入，才能对于这种极端场景保证逻辑上的正确性。

</div>

<div>

\

</div>

<div>

上面说到了一些碰到的一些比较经典的问题，但实际上每一个日常小、在这个环境下都可能是大问题

</div>

<div>

1、分析日志&拉取日志

</div>

<div>

高 qps下 精简日之后假设一条请求就打一条日志，一条日志1k 左右，假设qps
100w，一秒钟日志量976m，已经接近1G了，5分钟
285G，假设200台机器，单台机器1.4G +的数据。

</div>

<div>

拉取日志离线分析会把线上机器带宽打满。

</div>

<div>

线上直接grep后拉小日志，会把线上机器cpu打满。

</div>

<div>

2、consumer
服务需要向下游打到高qps，再加上内部待处理任务队列，concurrent
memory会使用率很高，有OOM风险并且时常触发GC，导致性能下降。

</div>

<div>

3、因为需要精准的控制对于下游业务系统的qps，常用限流策略比如说计数器、令牌桶等等会涉及一些锁操作（因为通常情况下
都是先检查后操作这样的竞态条件的情况），我们需要单台持续打到上万qps
这样的一个情况（没有机器），还有很多其他的锁操作，锁的存在是比较影响性能的。我们有几种策略：1）更改所使用的锁
2）优化取锁策略 3）优化JVM
等方式来解决，但是说起来简单，怎么做还是要根据具体场景来做的。并不是什么偏向锁之类的堆起来就好了，一味的乱用这些只会导致性能大幅度下降。

</div>

<div>

4、再来个最常见的，Redis 命令的使用也要非常谨慎，如何充分发挥Redis
的性能就需要就需要选择合适的命令，比如 一条HGETALL之类的命令就把redis
夯死了

</div>

<div>

类似的问题还有很多，我们后面慢慢看～

</div>
